{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":19990,"databundleVersionId":1472735,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":1443636,"sourceType":"datasetVersion","datasetId":844993},{"sourceId":43970252,"sourceType":"kernelVersion"},{"sourceId":263609062,"sourceType":"kernelVersion"}],"dockerImageVersionId":29995,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Motion Prediction Baseline — Inference with PyTorch & L5Kit\n\n_This notebook loads a trained BEV-based model and generates future trajectories for each agent in the test set._","metadata":{}},{"cell_type":"markdown","source":"## Notes\n\n- You need a trained model checkpoint (e.g. `model_state_last.pth`) saved from the training notebook.\n- A GPU is recommended for faster inference.\n- Parts of code from the [official example](https://github.com/lyft/l5kit/blob/master/examples/agent_motion_prediction/agent_motion_prediction.ipynb)\n- #### Version #1\n- Single mode baseline (resnet-18)\n- Trained for 25000 iterations (batch 32)\n- Input size 300px, history 1s (10 frames)\n- Adam (1e-3)\n- MSE Loss","metadata":{}},{"cell_type":"markdown","source":"## Imports & Setup\n\nWe import PyTorch, L5Kit, and utility libraries.  Paths for the dataset and where to save the submission are defined here.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport torch\n\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torchvision.models.resnet import resnet18\nfrom tqdm import tqdm\nfrom typing import Dict\n\nfrom l5kit.data import LocalDataManager, ChunkedDataset\nfrom l5kit.dataset import AgentDataset, EgoDataset\nfrom l5kit.evaluation import write_pred_csv\nfrom l5kit.rasterization import build_rasterizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DIR_INPUT = \"/kaggle/input/lyft-motion-prediction-autonomous-vehicles\"\n\nSINGLE_MODE_SUBMISSION = f\"{DIR_INPUT}/single_mode_sample_submission.csv\"\nMULTI_MODE_SUBMISSION = f\"{DIR_INPUT}/multi_mode_sample_submission.csv\"\n\n# Training notebook's output.\nWEIGHT_FILE = \"/kaggle/input/pytorch-baseline-train/model_state_last.pth\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configuration (cfg)\n\n- `model_params`: same as in training (history and future frames).\n- `raster_params`: raster size, pixel size, ego centre, map type.\n- `val_data_loader` / `test_data_loader`: specify zarr keys (`scenes/validate.zarr` or `scenes/sample.zarr`), batch size and workers.\n- `inference_params`: number of test scenes to process (can be limited for debug).\n\nThis dict mirrors the training config but points to the test split.","metadata":{}},{"cell_type":"code","source":"cfg = {\n    'format_version': 4,\n\n    'model_params': {\n        'model_architecture': 'resnet18',   \n        'history_num_frames': 10,\n        'history_step_size': 1,\n        'history_delta_time': 0.1,\n        'future_num_frames': 50,\n        'future_step_size': 1,\n        'future_delta_time': 0.1,\n    },\n\n    'raster_params': {\n        'raster_size': [224, 224],          \n        'pixel_size': [0.5, 0.5],\n        'ego_center': [0.25, 0.5],\n        'map_type': 'py_semantic',\n        'satellite_map_key': 'aerial_map/aerial_map.png',\n        'semantic_map_key': 'semantic_map/semantic_map.pb',\n        'dataset_meta_key': 'meta.json',\n        'filter_agents_threshold': 0.5,\n        'disable_traffic_light_faces': False \n    },\n\n    'test_data_loader': {\n        'key': 'scenes/test.zarr', \n        'batch_size': 8,\n        'shuffle': False,\n        'num_workers': 4,\n    }\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Root & LocalDataManager\n\nSet `L5KIT_DATA_FOLDER` and create a `LocalDataManager` so the loader can find Zarr files.","metadata":{}},{"cell_type":"code","source":"# set env variable for data\nos.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\ndm = LocalDataManager(None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Test Dataset & DataLoader\n\nWe build a rasteriser, open the test zarr, and create an `AgentDataset` or `EgoDataset` for inference.  The DataLoader wraps it with the chosen batch size and worker count.\n\nCheck `print(test_dataset)` to verify scene counts.","metadata":{}},{"cell_type":"code","source":"# ===== INIT DATASET\ntest_cfg = cfg[\"test_data_loader\"]\n\n# Rasterizer\nrasterizer = build_rasterizer(cfg, dm)\n\n# Test dataset/dataloader\ntest_zarr = ChunkedDataset(dm.require(test_cfg[\"key\"])).open()\ntest_mask = np.load(f\"{DIR_INPUT}/scenes/mask.npz\")[\"arr_0\"]\ntest_dataset = AgentDataset(cfg, test_zarr, rasterizer, agents_mask=test_mask)\ntest_dataloader = DataLoader(test_dataset,\n                             shuffle=test_cfg[\"shuffle\"],\n                             batch_size=test_cfg[\"batch_size\"],\n                             num_workers=test_cfg[\"num_workers\"])\n\n\nprint(test_dataloader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model\n\nSame architecture as training (e.g., ResNet-18).  \nFirst conv is widened for map + history channels; the head outputs **`2 × future_num_frames`** per sample.","metadata":{}},{"cell_type":"code","source":"class LyftModel(nn.Module):\n    \n    def __init__(self, cfg: Dict):\n        super().__init__()\n        \n        self.backbone = resnet18(pretrained=False)\n        \n        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n        num_in_channels = 3 + num_history_channels\n\n        self.backbone.conv1 = nn.Conv2d(\n            num_in_channels,\n            self.backbone.conv1.out_channels,\n            kernel_size=self.backbone.conv1.kernel_size,\n            stride=self.backbone.conv1.stride,\n            padding=self.backbone.conv1.padding,\n            bias=False,\n        )\n        \n        # This is 512 for resnet18 and resnet34;\n        # And it is 2048 for the other resnets\n        backbone_out_features = 512\n\n        # X, Y coords for the future positions (output shape: Bx50x2)\n        num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n\n        # You can add more layers here.\n        self.head = nn.Sequential(\n            # nn.Dropout(0.2),\n            nn.Linear(in_features=backbone_out_features, out_features=4096),\n        )\n\n        self.logit = nn.Linear(4096, out_features=num_targets)\n        \n    def forward(self, x):\n        x = self.backbone.conv1(x)\n        x = self.backbone.bn1(x)\n        x = self.backbone.relu(x)\n        x = self.backbone.maxpool(x)\n\n        x = self.backbone.layer1(x)\n        x = self.backbone.layer2(x)\n        x = self.backbone.layer3(x)\n        x = self.backbone.layer4(x)\n\n        x = self.backbone.avgpool(x)\n        x = torch.flatten(x, 1)\n        \n        x = self.head(x)\n        x = self.logit(x)\n        \n        return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Initialise & Load Weights\n\n- Instantiate `LyftModel(cfg)` and move to device.\n- Load the trained state dict from the training notebook’s output.\n- Set `model.eval()` and disable grads for inference.","metadata":{}},{"cell_type":"code","source":"# ==== INIT MODEL (inference) ====\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = LyftModel(cfg).to(device)\n\nif WEIGHT_FILE is not None:\n    state = torch.load(WEIGHT_FILE, map_location=device)\n    if isinstance(state, dict) and 'model_state_dict' in state:\n        model.load_state_dict(state['model_state_dict'])\n    else:\n        model.load_state_dict(state)\nelse:\n    raise FileNotFoundError(\"WEIGHT_FILE is None; attach your training notebook output or dataset and set WEIGHT_FILE path.\")\n\nmodel.eval()\ntorch.set_grad_enabled(False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Predicting\n\nLoop:\n1) Send batch `image` to device.  \n2) `outputs = model(inputs).reshape(targets.shape)` to get `(N, T, 2)`.  \n3) Accumulate `outputs`, `timestamp`, and `track_id` for the submission.","metadata":{}},{"cell_type":"code","source":"model.eval()\n\nfuture_coords_offsets_pd = []\ntimestamps = []\nagent_ids = []\n\nwith torch.no_grad():\n    dataiter = tqdm(test_dataloader)\n    \n    for data in dataiter:\n\n        inputs = data[\"image\"].to(device)\n        target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n        targets = data[\"target_positions\"].to(device)\n\n        outputs = model(inputs).reshape(targets.shape)\n        \n        future_coords_offsets_pd.append(outputs.cpu().numpy().copy())\n        timestamps.append(data[\"timestamp\"].numpy().copy())\n        agent_ids.append(data[\"track_id\"].numpy().copy())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save Submission\n\nConcatenate per-batch arrays and write a Kaggle-ready CSV:\n- `timestamps`\n- `track_ids`\n- `coords` (future `(x, y)` per step)\n\nVerify the file path is `/kaggle/working/submission.csv` for Kaggle submissions.","metadata":{}},{"cell_type":"code","source":"write_pred_csv('submission.csv',\n               timestamps=np.concatenate(timestamps),\n               track_ids=np.concatenate(agent_ids),\n               coords=np.concatenate(future_coords_offsets_pd))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}